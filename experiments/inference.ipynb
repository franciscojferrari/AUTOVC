{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "inference.ipynb",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmAMOL2WAsQ9",
        "outputId": "75d2dbed-2a62-43f9-aabe-70b62c309bab"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun May 23 20:56:35 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isRja8IfBDHV",
        "outputId": "aa213378-8d8c-484e-f34c-5eed96947eae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/auspicious3000/autovc.git\n",
        "%cd autovc\n",
        "!pip install wavenet_vocoder"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'autovc'...\n",
            "remote: Enumerating objects: 95, done.\u001b[K\n",
            "remote: Total 95 (delta 0), reused 0 (delta 0), pack-reused 95\u001b[K\n",
            "Unpacking objects: 100% (95/95), done.\n",
            "/content/autovc\n",
            "Collecting wavenet_vocoder\n",
            "  Downloading https://files.pythonhosted.org/packages/0a/da/47da119dbd3cfc0c80b75270e4bc7b49b678bd94600928fa243922ad65bc/wavenet_vocoder-0.1.1.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from wavenet_vocoder) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from wavenet_vocoder) (1.4.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from wavenet_vocoder) (1.8.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->wavenet_vocoder) (3.7.4.3)\n",
            "Building wheels for collected packages: wavenet-vocoder\n",
            "  Building wheel for wavenet-vocoder (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wavenet-vocoder: filename=wavenet_vocoder-0.1.1-cp37-none-any.whl size=12666 sha256=ec107769bc7a2e2f5b4d52552bbfe476c4cb769d2be3ea3565130463fb03beea\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/fc/21/02d3785b65dd072b110b44b9df98b8cbf72a89ddea424ff0d9\n",
            "Successfully built wavenet-vocoder\n",
            "Installing collected packages: wavenet-vocoder\n",
            "Successfully installed wavenet-vocoder-0.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEEpHdCgFZOQ",
        "outputId": "3868e9fe-54e0-43df-8a9d-dfcd652ee78e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRl4HyEGBPRB"
      },
      "source": [
        "## Test Pretrained AUTOVC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y76iFBqfBTR8"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "import numpy as np\n",
        "from math import ceil\n",
        "from model_vc import Generator\n",
        "\n",
        "\n",
        "def pad_seq(x, base=32):\n",
        "    len_out = int(base * ceil(float(x.shape[0])/base))\n",
        "    len_pad = len_out - x.shape[0]\n",
        "    assert len_pad >= 0\n",
        "    return np.pad(x, ((0,len_pad),(0,0)), 'constant'), len_pad\n",
        "\n",
        "device = 'cuda:0'\n",
        "G = Generator(32,256,512,32).eval().to(device)\n",
        "\n",
        "g_checkpoint = torch.load('/content/drive/MyDrive/KTH/DT2119/autovc/autovc.ckpt', map_location=device)\n",
        "G.load_state_dict(g_checkpoint['model'])\n",
        "\n",
        "metadata = pickle.load(open('metadata.pkl', \"rb\"))\n",
        "\n",
        "spect_vc = []\n",
        "\n",
        "for sbmt_i in metadata:\n",
        "             \n",
        "    x_org = sbmt_i[2]\n",
        "    x_org, len_pad = pad_seq(x_org)\n",
        "    uttr_org = torch.from_numpy(x_org[np.newaxis, :, :]).to(device)\n",
        "    emb_org = torch.from_numpy(sbmt_i[1][np.newaxis, :]).to(device)\n",
        "    \n",
        "    for sbmt_j in metadata:\n",
        "                   \n",
        "        emb_trg = torch.from_numpy(sbmt_j[1][np.newaxis, :]).to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            _, x_identic_psnt, _ = G(uttr_org, emb_org, emb_trg)\n",
        "            \n",
        "        if len_pad == 0:\n",
        "            uttr_trg = x_identic_psnt[0, 0, :, :].cpu().numpy()\n",
        "        else:\n",
        "            uttr_trg = x_identic_psnt[0, 0, :-len_pad, :].cpu().numpy()\n",
        "        \n",
        "        spect_vc.append( ('{}x{}'.format(sbmt_i[0], sbmt_j[0]), uttr_trg) )\n",
        "        \n",
        "        \n",
        "with open('results.pkl', 'wb') as handle:\n",
        "    pickle.dump(spect_vc, handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyoFkiDiCVZ1",
        "outputId": "1e108b7c-fe64-41af-b816-74040d782971"
      },
      "source": [
        "results = pickle.load(open('results.pkl', \"rb\"))\n",
        "len(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HRejeyZCsDJ",
        "outputId": "a84c0fc2-41c5-4ad1-b018-40fc9634505b"
      },
      "source": [
        "import torch\n",
        "import librosa\n",
        "import pickle\n",
        "from synthesis import build_model\n",
        "from synthesis import wavegen\n",
        "import soundfile as sf\n",
        "\n",
        "spect_vc = pickle.load(open('results.pkl', 'rb'))\n",
        "device = torch.device(\"cuda\")\n",
        "model = build_model().to(device)\n",
        "checkpoint = torch.load(\"/content/drive/MyDrive/KTH/DT2119/autovc/checkpoint_step001000000_ema.pth\", map_location=device)\n",
        "model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "for spect in spect_vc:\n",
        "    name = spect[0]\n",
        "    c = spect[1]\n",
        "    print(name)\n",
        "    waveform = wavegen(model, c=c)   \n",
        "    sf.write(\"/content/drive/MyDrive/KTH/DT2119/autovc/wavs/\" + name+'.wav', waveform, samplerate=16000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/23040 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "p225xp225\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 18%|█▊        | 4185/23040 [00:35<02:34, 121.67it/s]"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}