{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3612jvsc74a57bd02762fd0ccae7a6827a0de0868563b3d499c815e35640ddddc3d2dc7e9a34dcb9",
   "display_name": "Python 3.6.12 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import autovc as autovc\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "source": [
    "## Experiment with meta data from autovc repo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(90, 80)\n",
      "(89, 80)\n",
      "(75, 80)\n",
      "(109, 80)\n",
      "ipykernel_launcher:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "with open('metadata.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "# data = np.array(data)\n",
    "mel_specs, sp_emb = [], [] \n",
    "for d in data:\n",
    "    sp_emb.append(d[1])\n",
    "    print(d[2].shape)\n",
    "    mel_specs.append(d[2])\n",
    "mel_specs = np.array(mel_specs)\n",
    "sp_emb = np.array(sp_emb).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(4, 256)"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "sp_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "mel_specs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_mel_specs = tf.keras.preprocessing.sequence.pad_sequences(mel_specs, padding=\"post\", dtype='float32',value=-1.0, maxlen=128, truncating=\"post\")"
   ]
  },
  {
   "source": [
    "## Test Encoder with Functional API "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(4, 128, 80)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "padded_mel_specs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(4, 256)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "sp_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(autovc)\n",
    "encoder = autovc.Encoder()\n",
    "codes = encoder(padded_mel_specs, sp_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([ 4, 16, 32])>"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "tf.shape(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TensorShape([16, 4, 32])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "codes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TensorShape([4, 128, 32])"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "tf.keras.layers.UpSampling1D(8)(codes).shape"
   ]
  },
  {
   "source": [
    "## Test UpSampling with Functional API"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(autovc)\n",
    "upsample = autovc.UpSampling()\n",
    "output = upsample(codes, sp_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TensorShape([4, 128, 288])"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "source": [
    "## Test Decoder with Functional API "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(autovc)\n",
    "decoder = autovc.Decoder()\n",
    "mel_decoder = decoder(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TensorShape([4, 128, 80])"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "mel_decoder.shape"
   ]
  },
  {
   "source": [
    "## Test Posnet with Functional API"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(autovc)\n",
    "postnet = autovc.PostNet()\n",
    "mel_postnet = postnet(mel_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TensorShape([4, 128, 80])"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "mel_postnet.shape"
   ]
  },
  {
   "source": [
    "# Test End2End AUTOVC "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002277172F840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 42s 42s/step - loss_net: 0.9469 - loss_cd: 4.2460e-04\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 13s 13s/step - loss_net: 1.0962 - loss_cd: 0.0010\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 12s 12s/step - loss_net: 1.0176 - loss_cd: 5.5392e-04\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22771789940>"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "importlib.reload(autovc)\n",
    "model = autovc.AutoVC()\n",
    "model.compile(optimizer=\"Adam\")\n",
    "model.fit(x=(padded_mel_specs, sp_emb, sp_emb), epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGAAAAA8CAYAAACZ1L+0AAAABmJLR0QA/wD/AP+gvaeTAAAEGklEQVR4nO2cvU8ySxSHf1zv36BWFBYkVrTWlibTqYkmdBZrYWEPlS2Jdpq1ssAh0kGMFRQ2EBMVSjqXyqUaYm3OW7zOXvYLEcGTXM+TbCJnht2ZfebMTHCzGSIiCGz8w92A344IYEYEMCMCmPk3Gnh9fcXx8THe39852vO/ZWlpCaenp1hdXQ3FYxnQarVQrVZ/rGG/hWq1ilarFYvHMsByc3Oz0Ab9NjKZTGJc1gBmRAAzIoAZEcCMCGBGBDAjApgRAcyIAGZEADMigBkRwIwIYEYEMCMCmFmYgMPDw9TfwKdlNBp9+RyDwQCZTCZ2RPmsvNPpoFQqBeWlUgm9Xg/D4fDb/RpnIQIGgwEuLi4AAL1eb+bz3N/ff/k72WwWRIR2uw0AcF0XSY8+ERGazSa01rHyUqmEq6srFAoFEBGICEdHRxgMBlhZWZmtMyksRECtVkO9XgcAPDw8zHSO0WiEy8vLmduwsbGBcrmMp6en1DrPz8/Y3NwMxexIPz8/Ry6XC+LLy8tQSgVi5wZFqFQqlBCeGmMMFYtF+njiLvFcNj5eFo0Vi8VQbLyuMYa01kHcdV3yfT92Hc/zCABprSe209JutwkAtdvtiX2c5f4AoEqlEovPPQPu7u6wvb0N4G/6A/FpyPf92Pc8zwt9Pjk5Cf6mj2nAUigU8Pb2BiKC7/toNBo4ODjAaDQKnSObzaJcLmNvby9WNt5Oy+3tLQBgbW1tYh9pnk9zRo18JwOMMeQ4TvC52+0GIzRpRESvE40l1Wk2mwQgNOLtyE0a6f1+nwBQvV4PxZVSU7VpXuAnMuDx8RE7OzvB53w+DwBoNBpzu0atVgPwd062rK+vAwCur69j9XO5HBzHCa0nnU4H+/v7c2vTt4ga+U4GKKVi87Y9+v1+bERErxONTVPnszjRf5nYbDaJiMhxHPI8L1bPcRwCQMaY6Tr8BbDoDLCjij7ma3t0u10AmLgb+QpKKQDAcDiMlTmOk/idfD4Px3FwdnYWrEfZbDZWb2trCwDw8vIyl7ZORdTIrBngOE7qyFFKxeZczJgBdvczvlMxxoRGeBI2C5RSE+sppULrWBTP86hcLqeWp4GUDJiLAK11bEs3jt1Sji+SNt3t1GQXUgDBDbBTmu/7QaeNMYFQuxBrrSfetOg1J00xvu8HEqLTpud5oet+hYUJsDfNHtG5NVpu69jOYGyHopQirXXQQTtqi8ViqNO+75PrusH5tNZTzdvdbjdxRxbFGEP1ej0QZjPHdd3EtWMaFpoBwuekCZBfQ5kRAcyIAGZEADMigBkRwIwIYEYEMCMCmBEBzIgAZkQAMyKAGRHAjAhgRgQwIwKYSX1byu7u7k+249eS+fh3WYC8sGkxpL2wKSZA+FlkDWBGBDAjApgRAcz8AYrY4e/CjUgFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "importlib.reload(autovc)\n",
    "model = autovc.AutoVC()\n",
    "model.compile(optimizer=\"Adam\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.build(input_shape=[(128,80), (256,)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_post = model(padded_mel_specs, sp_emb, sp_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TensorShape([4, 128, 80])"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "mel_post.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"AutoVC\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nencoder (Encoder)            multiple                  3008256   \n_________________________________________________________________\ndecoder_20 (Decoder)         multiple                  26529360  \n_________________________________________________________________\npostnet (PostNet)            multiple                  5665680   \n_________________________________________________________________\nupsampling (UpSampling)      multiple                  0         \n=================================================================\nTotal params: 35,203,296\nTrainable params: 35,191,872\nNon-trainable params: 11,424\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGAAAAA8CAYAAACZ1L+0AAAABmJLR0QA/wD/AP+gvaeTAAAEGklEQVR4nO2cvU8ySxSHf1zv36BWFBYkVrTWlibTqYkmdBZrYWEPlS2Jdpq1ssAh0kGMFRQ2EBMVSjqXyqUaYm3OW7zOXvYLEcGTXM+TbCJnht2ZfebMTHCzGSIiCGz8w92A344IYEYEMCMCmPk3Gnh9fcXx8THe39852vO/ZWlpCaenp1hdXQ3FYxnQarVQrVZ/rGG/hWq1ilarFYvHMsByc3Oz0Ab9NjKZTGJc1gBmRAAzIoAZEcCMCGBGBDAjApgRAcyIAGZEADMigBkRwIwIYEYEMCMCmFmYgMPDw9TfwKdlNBp9+RyDwQCZTCZ2RPmsvNPpoFQqBeWlUgm9Xg/D4fDb/RpnIQIGgwEuLi4AAL1eb+bz3N/ff/k72WwWRIR2uw0AcF0XSY8+ERGazSa01rHyUqmEq6srFAoFEBGICEdHRxgMBlhZWZmtMyksRECtVkO9XgcAPDw8zHSO0WiEy8vLmduwsbGBcrmMp6en1DrPz8/Y3NwMxexIPz8/Ry6XC+LLy8tQSgVi5wZFqFQqlBCeGmMMFYtF+njiLvFcNj5eFo0Vi8VQbLyuMYa01kHcdV3yfT92Hc/zCABprSe209JutwkAtdvtiX2c5f4AoEqlEovPPQPu7u6wvb0N4G/6A/FpyPf92Pc8zwt9Pjk5Cf6mj2nAUigU8Pb2BiKC7/toNBo4ODjAaDQKnSObzaJcLmNvby9WNt5Oy+3tLQBgbW1tYh9pnk9zRo18JwOMMeQ4TvC52+0GIzRpRESvE40l1Wk2mwQgNOLtyE0a6f1+nwBQvV4PxZVSU7VpXuAnMuDx8RE7OzvB53w+DwBoNBpzu0atVgPwd062rK+vAwCur69j9XO5HBzHCa0nnU4H+/v7c2vTt4ga+U4GKKVi87Y9+v1+bERErxONTVPnszjRf5nYbDaJiMhxHPI8L1bPcRwCQMaY6Tr8BbDoDLCjij7ma3t0u10AmLgb+QpKKQDAcDiMlTmOk/idfD4Px3FwdnYWrEfZbDZWb2trCwDw8vIyl7ZORdTIrBngOE7qyFFKxeZczJgBdvczvlMxxoRGeBI2C5RSE+sppULrWBTP86hcLqeWp4GUDJiLAK11bEs3jt1Sji+SNt3t1GQXUgDBDbBTmu/7QaeNMYFQuxBrrSfetOg1J00xvu8HEqLTpud5oet+hYUJsDfNHtG5NVpu69jOYGyHopQirXXQQTtqi8ViqNO+75PrusH5tNZTzdvdbjdxRxbFGEP1ej0QZjPHdd3EtWMaFpoBwuekCZBfQ5kRAcyIAGZEADMigBkRwIwIYEYEMCMCmBEBzIgAZkQAMyKAGRHAjAhgRgQwIwKYSX1byu7u7k+249eS+fh3WYC8sGkxpL2wKSZA+FlkDWBGBDAjApgRAcz8AYrY4e/CjUgFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model, \"model.png\")"
   ]
  }
 ]
}